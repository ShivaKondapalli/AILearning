{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation with LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShivaKondapalli/AILearning/blob/master/Text_Generation_with_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1ZCoVD_0nog",
        "colab_type": "text"
      },
      "source": [
        "In this Notebook, we shall generate text from **Francis Ford Coppola's Masterpiece Apocalypse Now**. We will use \n",
        "an **LSTM network** with 2 layers to build  a **Languague model**.  We'll surely have bots writing scripts for us in the \n",
        "future!\n",
        "\n",
        "First, let's get a high level walkthrough of what the model: **LSTM** , and an exciting techinque in NLP called LANGAUGE MODELLING are all about.  We will then see how how the two are used to generate text. \n",
        "\n",
        "The code reference for this, almost verbatim the same is : \"[link text](https://github.com/LeanManager/NLP-PyTorch/blob/master/Character-Level%20LSTM%20with%20PyTorch.ipynb)\" \n",
        "\n",
        "\n",
        "**LSTM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzAlda2I69d4",
        "colab_type": "text"
      },
      "source": [
        "Install Pypdf2 to extract data from pdf file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D38jGWA06zIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install PyPDF2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3z8Di5ew0Tq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import all libraries\n",
        "\n",
        "import torch\n",
        "from PyPDF2 import PdfFileReader\n",
        "import re\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5Ncixiaxfhi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4085cb8a-1f3f-42a1-a472-4b923a272780"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  print('The power of the GPU is with you!')\n",
        "else:\n",
        "  print('Trainingi your model forever!')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The power of the GPU is with you!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qILy8VYyNxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"Apocalypse_Now.pdf\"\n",
        "\n",
        "file_object = open(path, 'rb')\n",
        "\n",
        "def read_data(file_object):\n",
        "\n",
        "    data = PdfFileReader(file_object)\n",
        "\n",
        "    if data.isEncrypted:\n",
        "        data.decrypt('')\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def extract_all_data(data):\n",
        "\n",
        "    text = ''\n",
        "\n",
        "    for i in range(1, data.numPages):\n",
        "\n",
        "        pageobj = data.getPage(i)\n",
        "\n",
        "        text += pageobj.extractText()\n",
        "\n",
        "    return text\n",
        "  \n",
        "\n",
        "# Extracting file object\n",
        "file_object = read_data(\"Apocalypse_Now.pdf\")\n",
        "\n",
        "# reading data into data_list, every page of our pdf is a string\n",
        "text = extract_all_data(file_object)  # length = 138, numofPages."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuGutukJ6gG7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5b611455-e6a4-48bf-da08-8ddd4852f12d"
      },
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"function to convert text to lowercase, retain alphabets, replace hyphenation with space\"\"\"\n",
        "\n",
        "    text = text.replace('-', '')\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate({ord(i): None for i in '!\"?();Â´'})\n",
        "    text = text.replace('--', '')\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "Clean_Text = clean_text(text)\n",
        "\n",
        "chars = tuple(set(Clean_Text))\n",
        "\n",
        "int2char = dict(enumerate(chars))\n",
        "\n",
        "char2int = {w: i for i, w in int2char.items()}\n",
        "\n",
        "# each word in tokens converted to an integer.\n",
        "encoded = np.array([char2int[char] for char in Clean_Text])\n",
        "print(encoded)\n",
        "print(encoded.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[29 29 26 ... 20 56 18]\n",
            "(158684,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA-e3HjWA-nG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bf3428be-ead0-4085-cc3a-71f2170d7884"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "\n",
        "    # Initialize the encoded array\n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "\n",
        "    # # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "\n",
        "    # # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "    return one_hot\n",
        " \n",
        "ch = np.array([[char2int['w']]])\n",
        "print(ch)\n",
        "print(ch.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2]]\n",
            "(1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEKpdXRhBW94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, n_seqs, n_steps):\n",
        "\n",
        "    batch_size = n_seqs * n_steps\n",
        "\n",
        "    # total number of characters in our batch, in vision, number of images in a batch\n",
        "    # is batch size, 64, 32 and so on. total # of batches is the length of training set/batch_size.\n",
        "\n",
        "    n_batches = len(arr)//batch_size  # total num of batches to make from encoded array.\n",
        "\n",
        "    # following is total characters to keep from encoded array to have full batches which are evenly spaced.\n",
        "\n",
        "    # number of batches * characters per batch will give the number of characters to slice form our encoded array.\n",
        "    arr = arr[:n_batches * batch_size]\n",
        "    # print(arr)\n",
        "    # print(arr.shape)\n",
        "\n",
        "    # we will be splitting this array enc into N number of batches each having seq_len number of characters.\n",
        "\n",
        "    arr = arr.reshape((n_seqs, -1))\n",
        "    # print(enc)\n",
        "    # print(enc.shape)\n",
        "\n",
        "    # sq_len * number of batches, this should be equal to the second dimension of our array.\n",
        "    # print(40 * 396) # product of this with n_seqs_per_batch gives total number of characters.\n",
        "\n",
        "    # 10 * 40 window 10 * 15480\n",
        "\n",
        "    # we now need to split our features\n",
        "\n",
        "    for n in range(0, arr.shape[1], n_steps):\n",
        "\n",
        "        x = arr[:, n:n+n_steps]\n",
        "\n",
        "        y = np.zeros_like(x)\n",
        "\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n + n_steps]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "\n",
        "        yield x, y\n",
        "        \n",
        "batches = get_batches(encoded, 10, 40)\n",
        "x, y = next(batches)\n",
        "# five rows,\n",
        "print(x[:5])\n",
        "print('')\n",
        "print(y[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CQR06a6BnTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class CharLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, chars, n_steps=100, hidden_size=256, n_layers=2, drop_out=0.5, lr= 0.001):\n",
        "\n",
        "        super(CharLSTM, self).__init__()\n",
        "\n",
        "        # Set all the hyperparameters of your network\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.drop_out = drop_out\n",
        "        self.lr = lr\n",
        "\n",
        "        # set vocabulary and get indices for these\n",
        "        self.chars = chars\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {w : i for i, w in self.int2char.items()}\n",
        "\n",
        "        # define the lstm network, this outputs the next char and cell state and hidden state\n",
        "        self.lstm = nn.LSTM(input_size=len(self.chars), hidden_size=hidden_size, num_layers=n_layers, dropout=drop_out,\n",
        "                       batch_first=True)\n",
        "        # add dropout\n",
        "        self.drop_out = nn.Dropout(drop_out)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, len(chars))\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, x, h_0):\n",
        "        \"\"\"compute current and hidden units, stack outputs and pass it to linear layer\"\"\"\n",
        "\n",
        "        x, (h, c) = self.lstm(x, h_0)\n",
        "\n",
        "        x = self.drop_out(x)\n",
        "\n",
        "        # reshape x: stack the predicted and hidden state outputs\n",
        "        # cause fully-connected.\n",
        "        x = x.view(x.size()[0]*x.size()[1], self.hidden_size)\n",
        "\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x, (h, c)\n",
        "\n",
        "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
        "        \"\"\"given a character, predict the next character in the sequence.\"\"\"\n",
        "\n",
        "        # Check for cuda\n",
        "        if cuda:\n",
        "            self.cuda()\n",
        "        else:\n",
        "            self.cpu()\n",
        "\n",
        "        # Initialize hidden state\n",
        "        if h is None:\n",
        "            h = self.init_hidden(1)\n",
        "\n",
        "        # get the integer of character.\n",
        "        ch = np.array([[self.char2int[char]]])\n",
        "\n",
        "        # one_hot_encode\n",
        "        one_hot = one_hot_encode(ch, len(self.chars))\n",
        "\n",
        "        # convet to tensor\n",
        "        one_hot_torch = torch.from_numpy(one_hot)\n",
        "\n",
        "        # move input to cuda if available.\n",
        "        if cuda:\n",
        "            one_hot_torch.cuda()\n",
        "\n",
        "        # create a tuple of the hidden state\n",
        "        # this is what LSTM expects\n",
        "        h = tuple([each.data for each in h])\n",
        "        out, h = self.forward(ch, h)\n",
        "\n",
        "        # Prob distribution over all the characters\n",
        "        probs = F.softmax(out, dim=1).data\n",
        "\n",
        "        # convert back prob to cpu if model was\n",
        "        # set to gpu\n",
        "        if cuda:\n",
        "            probs.cpu()\n",
        "\n",
        "        # if top number of preds to get\n",
        "        # wasn't pass, take the distribution over whole character length\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(self.chars))\n",
        "        else:\n",
        "            probs, top_ch = probs.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "        # reduce dims of size 1\n",
        "        probs = probs.numpy().squeeze()\n",
        "\n",
        "        # sample from top_char with each probs of each character being\n",
        "        # the char with higher prob will be chosen since dividing the\n",
        "        # highest value with probs.sum() is what is the best.\n",
        "        char = np.random.choice(top_ch, p=probs/probs.sum())\n",
        "\n",
        "        return self.char2int[char], h\n",
        "\n",
        "    def init_weights(self):\n",
        "\n",
        "        initrange = 0.1\n",
        "\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-1, 1)\n",
        "\n",
        "    def init_hidden(self, n_seqs):\n",
        "\n",
        "        weight = next(self.parameters()).data\n",
        "        return (weight.new(self.n_layers, n_seqs, self.hidden_size).zero_(),\n",
        "                weight.new(self.n_layers, n_seqs, self.hidden_size).zero_())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apZirO61O9Ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, data, epochs=10, n_seqs=10, n_steps=40, lr=0.001, clip=5, val_frac= 0.1, cuda=False,\n",
        "          print_every=10):\n",
        "\n",
        "    \"\"\" model: the model to b trained\n",
        "        data: the data on which we train\n",
        "        epochs: number of epochs to train for\n",
        "        n_seqs\" number of sequences in our batch\n",
        "        n_steps: time step for each sequence\n",
        "        lr: learning rate\n",
        "        clip: value used to clip the network gradient to prevent exploding gradeint.\n",
        "        val_frac: the fraction of data used for validation\n",
        "        print_every: the number of seconds for which we print out model statistics\n",
        "    \"\"\"\n",
        "\n",
        "    # change model to train mode\n",
        "    model.train()\n",
        "\n",
        "    # define optimizer and loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # trin and validation split\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "\n",
        "    # check for cuda move model to cuda\n",
        "    if cuda:\n",
        "        model.cuda()\n",
        "\n",
        "    counter = 0\n",
        "    n_chars = len(model.chars)\n",
        "\n",
        "    # loop over epochs\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # initialize hidden layer of the model\n",
        "        h = model.init_hidden(n_seqs)\n",
        "\n",
        "        # loop over batches\n",
        "        for x, y in get_batches(data, n_seqs, n_steps):\n",
        "\n",
        "            counter += 1\n",
        "\n",
        "            # one hot encode\n",
        "            one_hot = one_hot_encode(x, n_chars)\n",
        "\n",
        "            # convert to tensors\n",
        "            inputs, targets = torch.from_numpy(one_hot), torch.from_numpy(y)\n",
        "\n",
        "            # move inputs and targets to cuda\n",
        "            if cuda:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "                \n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero out gradient to prevent accumulation\n",
        "            model.zero_grad()\n",
        "\n",
        "            # get output and hidden\n",
        "            out, h = model(inputs, h)\n",
        "            loss = criterion(out, targets.view(n_seqs*n_steps).type(torch.cuda.LongTensor))\n",
        "\n",
        "            # backpropogate loss\n",
        "            loss.backward()\n",
        "\n",
        "            # use gradient clipping to prevent exploding gradient\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "            # take a step in the los surface\n",
        "            optimizer.step()\n",
        "\n",
        "            if counter % print_every ==0:\n",
        "\n",
        "                # initilize hidden state for validation\n",
        "                val_hidden = model.init_hidden(n_steps)\n",
        "                val_losses = []\n",
        "\n",
        "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
        "\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "                    val_hidden = tuple([each.data for each in val_hidden])\n",
        "\n",
        "                    if cuda:\n",
        "                        inputs, targets = x.cuda(), y.cuda()\n",
        "\n",
        "                    out, val_hidden = model.forward(inputs, val_hidden)\n",
        "\n",
        "                    val_loss = criterion(out, targets.view(n_seqs*n_steps).type(torch.LongTensor))\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                    print('Epoch:'.format(epoch+1),\n",
        "                          'Steps:'.format(counter),\n",
        "                          'train loss {.:4f}'.format(loss.item),\n",
        "                          'val loss {.:4f}'.format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA8rpIEtIDM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if model in locals():\n",
        "  del model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cocjEMcJCoQ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c30aa96e-e6a8-4687-c055-f1be98eb754f"
      },
      "source": [
        "model = CharLSTM(chars, hidden_size=512, n_layers=2)\n",
        "model"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharLSTM(\n",
              "  (lstm): LSTM(58, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
              "  (drop_out): Dropout(p=0.5)\n",
              "  (fc): Linear(in_features=512, out_features=58, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4uIPLG0C4W3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_seqs = 128\n",
        "n_steps = 100 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttvcu9DVKO03",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "3fcd3d25-adc7-42e6-a2ad-24d4dce7dd35"
      },
      "source": [
        "train(model, encoded, epochs=25, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=True, print_every=10)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-b84353334257>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_seqs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-91-ef01e707455d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, epochs, n_seqs, n_steps, lr, clip, val_frac, cuda, print_every)\u001b[0m\n\u001b[1;32m     87\u001b[0m                         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_seqs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-97102b771ded>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h_0)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;34m\"\"\"compute current and hidden units, stack outputs and pass it to linear layer\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         self.check_hidden_size(hidden[0], expected_hidden_size,\n\u001b[0;32m--> 494\u001b[0;31m                                'Expected hidden[0] size {}, got {}')\n\u001b[0m\u001b[1;32m    495\u001b[0m         self.check_hidden_size(hidden[1], expected_hidden_size,\n\u001b[1;32m    496\u001b[0m                                'Expected hidden[1] size {}, got {}')\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# type: (Tensor, Tuple[int, int, int], str) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (2, 128, 512), got (2, 100, 512)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Hn87xwwC-nl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = \"lstm.net\"\n",
        "\n",
        "checkpoint = {'n_hidden': model.hidden_size, \n",
        "             'n_layers': model.n_layers, \n",
        "             'droput': model.drop_out, \n",
        "             'state_dict': model.state_dict(), \n",
        "             'toekns': model.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqJhQ6ZTSFMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
        "        \n",
        "    if cuda:\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "\n",
        "    net.eval()\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    \n",
        "    h = net.init_hidden(1)\n",
        "    \n",
        "    for ch in prime:\n",
        "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        \n",
        "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y9Wn7hvSGC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}